# common configuration for test runs
spark.ui.enabled = false
spark.logConf = true
spark.driver.cores = 1
spark.sql.shuffle.partitions = 1

# local config (this will be overridden if is-remote=true
spark.master = local[1]
spark.driver.host" = localhost
spark.4sq.test.is-remote = false

# AWS credentials for assumed role. If you are not using an assumed role then comment this section out.
# ref: https://www.aloneguid.uk/posts/2021/03/spark-s3-assume-role/
# ref: https://docs.aws.amazon.com/sdkref/latest/guide/feature-sts-regionalized-endpoints.html
# ref: https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/assumed_roles.html
spark.hadoop.fs.s3a.aws.credentials.provider = org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider
spark.hadoop.fs.s3a.assumed.role.arn = arn:aws:iam::798457485286:role/foursquare-data-sharing
spark.hadoop.fs.s3a.assumed.role.session.name = kinetica-spark
spark.hadoop.fs.s3a.assumed.role.session.duration = 3600

# The endpoint options below may be needed if there is no aws config to work around a bug
spark.hadoop.fs.s3a.assumed.role.sts.endpoint = sts.us-east-1.amazonaws.com
spark.hadoop.fs.s3a.assumed.role.sts.endpoint.region = us-east-1

# for transform job
spark.4sq.transform.dest-table = foursquare.places_test
spark.4sq.transform.source = s3a://4sq-partner-grapheval/datasets/generated/foursquareweb/fsq-graph-place/place/dt=2023-03-14/
spark.4sq.transform.limit = 1000
